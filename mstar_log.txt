Filling queue with 20000 SVHN images before starting to train. This will take a few minutes.
2016-01-19 14:39:07.987914: step 0, loss = 11.16 (18.1 examples/sec; 7.076 sec/batch)
2016-01-19 14:39:34.483111: step 10, loss = 11.03 (54.3 examples/sec; 2.357 sec/batch)
2016-01-19 14:39:58.055623: step 20, loss = 10.96 (52.6 examples/sec; 2.432 sec/batch)
2016-01-19 14:40:21.435324: step 30, loss = 10.87 (55.5 examples/sec; 2.307 sec/batch)
2016-01-19 14:40:44.501903: step 40, loss = 10.78 (55.2 examples/sec; 2.320 sec/batch)
2016-01-19 14:41:07.502365: step 50, loss = 10.71 (56.1 examples/sec; 2.280 sec/batch)
2016-01-19 14:41:30.408205: step 60, loss = 10.63 (55.8 examples/sec; 2.293 sec/batch)
2016-01-19 14:41:53.345829: step 70, loss = 10.59 (55.9 examples/sec; 2.288 sec/batch)
2016-01-19 14:42:17.568100: step 80, loss = 10.49 (54.4 examples/sec; 2.352 sec/batch)
2016-01-19 14:42:41.857295: step 90, loss = 10.38 (55.0 examples/sec; 2.328 sec/batch)
2016-01-19 14:43:08.641526: step 100, loss = 10.36 (44.9 examples/sec; 2.849 sec/batch)
2016-01-19 14:43:36.791607: step 110, loss = 10.25 (47.8 examples/sec; 2.679 sec/batch)
2016-01-19 14:44:01.916882: step 120, loss = 10.20 (53.3 examples/sec; 2.403 sec/batch)
2016-01-19 14:44:26.935338: step 130, loss = 10.09 (42.5 examples/sec; 3.012 sec/batch)
2016-01-19 14:44:50.393496: step 140, loss = 9.97 (53.0 examples/sec; 2.414 sec/batch)
2016-01-19 14:45:14.189231: step 150, loss = 9.93 (55.7 examples/sec; 2.299 sec/batch)
2016-01-19 14:45:37.459321: step 160, loss = 9.79 (55.0 examples/sec; 2.326 sec/batch)
2016-01-19 14:46:00.569963: step 170, loss = 9.67 (55.1 examples/sec; 2.325 sec/batch)
2016-01-19 14:46:23.691690: step 180, loss = 9.58 (55.7 examples/sec; 2.296 sec/batch)
2016-01-19 14:46:46.747777: step 190, loss = 9.59 (55.4 examples/sec; 2.311 sec/batch)
2016-01-19 14:47:09.862749: step 200, loss = 9.58 (55.6 examples/sec; 2.303 sec/batch)
2016-01-19 14:47:35.818537: step 210, loss = 9.33 (54.6 examples/sec; 2.345 sec/batch)
2016-01-19 14:48:00.913834: step 220, loss = 9.25 (52.0 examples/sec; 2.459 sec/batch)
2016-01-19 14:48:24.350862: step 230, loss = 9.65 (54.9 examples/sec; 2.331 sec/batch)
2016-01-19 14:48:47.636956: step 240, loss = 9.12 (55.2 examples/sec; 2.318 sec/batch)
2016-01-19 14:49:10.943473: step 250, loss = 9.06 (54.6 examples/sec; 2.343 sec/batch)
2016-01-19 14:49:34.333434: step 260, loss = 9.02 (54.6 examples/sec; 2.346 sec/batch)
2016-01-19 14:49:57.784007: step 270, loss = 8.92 (54.7 examples/sec; 2.339 sec/batch)
2016-01-19 14:50:21.168552: step 280, loss = 8.74 (55.0 examples/sec; 2.326 sec/batch)
2016-01-19 14:50:44.587349: step 290, loss = 8.70 (54.6 examples/sec; 2.345 sec/batch)
2016-01-19 14:51:08.020655: step 300, loss = 8.63 (54.9 examples/sec; 2.332 sec/batch)
2016-01-19 14:51:34.172132: step 310, loss = 8.51 (55.1 examples/sec; 2.322 sec/batch)
2016-01-19 14:51:57.690450: step 320, loss = 8.49 (54.2 examples/sec; 2.362 sec/batch)
2016-01-19 14:52:21.224157: step 330, loss = 8.51 (54.7 examples/sec; 2.340 sec/batch)
2016-01-19 14:52:44.734920: step 340, loss = 8.53 (54.6 examples/sec; 2.346 sec/batch)
2016-01-19 14:53:08.235262: step 350, loss = 8.29 (54.3 examples/sec; 2.358 sec/batch)
2016-01-19 14:53:31.625880: step 360, loss = 8.18 (54.8 examples/sec; 2.336 sec/batch)
2016-01-19 14:53:54.960039: step 370, loss = 8.57 (54.9 examples/sec; 2.333 sec/batch)
2016-01-19 14:54:18.276317: step 380, loss = 7.98 (54.9 examples/sec; 2.332 sec/batch)
2016-01-19 14:54:43.621061: step 390, loss = 8.06 (48.7 examples/sec; 2.630 sec/batch)
2016-01-19 14:55:08.477616: step 400, loss = 7.80 (50.3 examples/sec; 2.543 sec/batch)
2016-01-19 14:55:35.376841: step 410, loss = 7.87 (54.5 examples/sec; 2.347 sec/batch)
2016-01-19 14:56:01.074684: step 420, loss = 7.79 (52.2 examples/sec; 2.451 sec/batch)
2016-01-19 14:56:26.906137: step 430, loss = 7.77 (50.4 examples/sec; 2.538 sec/batch)
2016-01-19 14:56:50.268727: step 440, loss = 7.82 (54.7 examples/sec; 2.339 sec/batch)
2016-01-19 14:57:13.572390: step 450, loss = 7.52 (55.2 examples/sec; 2.319 sec/batch)
2016-01-19 14:57:36.872580: step 460, loss = 7.58 (55.2 examples/sec; 2.317 sec/batch)
2016-01-19 14:58:00.235120: step 470, loss = 7.37 (54.6 examples/sec; 2.343 sec/batch)
2016-01-19 14:58:23.543775: step 480, loss = 7.34 (55.2 examples/sec; 2.320 sec/batch)
2016-01-19 14:58:46.856835: step 490, loss = 7.38 (55.7 examples/sec; 2.298 sec/batch)
2016-01-19 14:59:10.072471: step 500, loss = 7.17 (54.7 examples/sec; 2.338 sec/batch)
2016-01-19 14:59:36.013841: step 510, loss = 7.15 (54.6 examples/sec; 2.346 sec/batch)
2016-01-19 14:59:59.221095: step 520, loss = 7.07 (55.3 examples/sec; 2.313 sec/batch)
2016-01-19 15:00:22.438556: step 530, loss = 7.08 (55.3 examples/sec; 2.313 sec/batch)
2016-01-19 15:00:45.625456: step 540, loss = 6.96 (55.5 examples/sec; 2.306 sec/batch)
2016-01-19 15:01:08.774062: step 550, loss = 6.96 (55.3 examples/sec; 2.315 sec/batch)
2016-01-19 15:01:32.161658: step 560, loss = 6.93 (52.8 examples/sec; 2.426 sec/batch)
2016-01-19 15:01:56.774086: step 570, loss = 6.79 (53.0 examples/sec; 2.414 sec/batch)
2016-01-19 15:02:19.997346: step 580, loss = 6.73 (55.1 examples/sec; 2.322 sec/batch)
2016-01-19 15:02:43.181169: step 590, loss = 6.79 (55.5 examples/sec; 2.308 sec/batch)
2016-01-19 15:03:06.304038: step 600, loss = 6.68 (55.1 examples/sec; 2.322 sec/batch)
2016-01-19 15:03:32.134677: step 610, loss = 6.53 (55.0 examples/sec; 2.325 sec/batch)
2016-01-19 15:03:55.305869: step 620, loss = 6.54 (55.2 examples/sec; 2.318 sec/batch)
2016-01-19 15:04:18.480565: step 630, loss = 6.51 (54.9 examples/sec; 2.330 sec/batch)
2016-01-19 15:04:41.715000: step 640, loss = 6.38 (55.5 examples/sec; 2.306 sec/batch)
2016-01-19 15:05:04.892758: step 650, loss = 7.78 (55.2 examples/sec; 2.320 sec/batch)
2016-01-19 15:05:27.967224: step 660, loss = 6.35 (55.7 examples/sec; 2.299 sec/batch)
2016-01-19 15:05:51.069715: step 670, loss = 6.26 (55.3 examples/sec; 2.314 sec/batch)
2016-01-19 15:06:14.254926: step 680, loss = 6.43 (55.3 examples/sec; 2.314 sec/batch)
2016-01-19 15:06:37.390053: step 690, loss = 6.29 (55.4 examples/sec; 2.311 sec/batch)
2016-01-19 15:07:00.728002: step 700, loss = 6.09 (52.7 examples/sec; 2.428 sec/batch)
2016-01-19 15:07:26.703730: step 710, loss = 6.14 (55.6 examples/sec; 2.301 sec/batch)
2016-01-19 15:07:49.837742: step 720, loss = 5.96 (55.3 examples/sec; 2.315 sec/batch)
2016-01-19 15:08:13.024170: step 730, loss = 5.98 (55.1 examples/sec; 2.323 sec/batch)
2016-01-19 15:08:36.138578: step 740, loss = 5.96 (55.1 examples/sec; 2.321 sec/batch)
2016-01-19 15:08:59.276418: step 750, loss = 5.91 (55.3 examples/sec; 2.315 sec/batch)
2016-01-19 15:09:22.476067: step 760, loss = 5.82 (55.1 examples/sec; 2.321 sec/batch)
2016-01-19 15:09:46.598976: step 770, loss = 6.00 (55.0 examples/sec; 2.327 sec/batch)
2016-01-19 15:10:10.259408: step 780, loss = 5.69 (52.4 examples/sec; 2.441 sec/batch)
2016-01-19 15:10:33.628896: step 790, loss = 6.17 (54.8 examples/sec; 2.334 sec/batch)
2016-01-19 15:10:56.727662: step 800, loss = 5.66 (55.3 examples/sec; 2.314 sec/batch)
2016-01-19 15:11:22.630288: step 810, loss = 5.58 (55.4 examples/sec; 2.311 sec/batch)
2016-01-19 15:11:45.728376: step 820, loss = 5.52 (55.6 examples/sec; 2.303 sec/batch)
2016-01-19 15:12:08.864937: step 830, loss = 5.64 (55.2 examples/sec; 2.320 sec/batch)
2016-01-19 15:12:31.987661: step 840, loss = 5.45 (55.6 examples/sec; 2.304 sec/batch)
2016-01-19 15:12:55.185115: step 850, loss = 5.45 (54.9 examples/sec; 2.330 sec/batch)
2016-01-19 15:13:18.276780: step 860, loss = 5.39 (55.2 examples/sec; 2.317 sec/batch)
2016-01-19 15:13:41.366130: step 870, loss = 5.33 (55.7 examples/sec; 2.297 sec/batch)
2016-01-19 15:14:04.484036: step 880, loss = 5.27 (55.4 examples/sec; 2.309 sec/batch)
2016-01-19 15:14:27.619739: step 890, loss = 5.31 (55.6 examples/sec; 2.303 sec/batch)
2016-01-19 15:14:50.652069: step 900, loss = 5.23 (55.3 examples/sec; 2.316 sec/batch)
2016-01-19 15:15:16.393985: step 910, loss = 5.14 (55.8 examples/sec; 2.296 sec/batch)
2016-01-19 15:15:39.426598: step 920, loss = 5.09 (55.6 examples/sec; 2.303 sec/batch)
2016-01-19 15:16:02.523314: step 930, loss = 5.06 (55.1 examples/sec; 2.324 sec/batch)
2016-01-19 15:16:25.695714: step 940, loss = 5.02 (55.2 examples/sec; 2.317 sec/batch)
2016-01-19 15:16:49.448343: step 950, loss = 5.00 (54.8 examples/sec; 2.336 sec/batch)
2016-01-19 15:17:12.659294: step 960, loss = 4.92 (55.1 examples/sec; 2.321 sec/batch)
2016-01-19 15:17:35.917512: step 970, loss = 4.88 (55.0 examples/sec; 2.328 sec/batch)
2016-01-19 15:17:59.066394: step 980, loss = 4.90 (55.1 examples/sec; 2.321 sec/batch)
2016-01-19 15:18:22.321409: step 990, loss = 4.79 (55.1 examples/sec; 2.322 sec/batch)
2016-01-19 15:18:45.484120: step 1000, loss = 4.77 (55.3 examples/sec; 2.316 sec/batch)
2016-01-19 15:19:11.455794: step 1010, loss = 4.77 (55.8 examples/sec; 2.293 sec/batch)
2016-01-19 15:19:34.686351: step 1020, loss = 4.91 (56.7 examples/sec; 2.258 sec/batch)
2016-01-19 15:19:57.637698: step 1030, loss = 4.72 (55.4 examples/sec; 2.312 sec/batch)
2016-01-19 15:20:20.583685: step 1040, loss = 4.67 (56.0 examples/sec; 2.285 sec/batch)
2016-01-19 15:20:43.641503: step 1050, loss = 4.64 (55.7 examples/sec; 2.298 sec/batch)
2016-01-19 15:21:06.670533: step 1060, loss = 4.60 (55.7 examples/sec; 2.298 sec/batch)
2016-01-19 15:21:29.754899: step 1070, loss = 4.59 (55.1 examples/sec; 2.321 sec/batch)
2016-01-19 15:21:53.788724: step 1080, loss = 4.50 (42.3 examples/sec; 3.029 sec/batch)
2016-01-19 15:22:18.355139: step 1090, loss = 4.47 (54.4 examples/sec; 2.355 sec/batch)
2016-01-19 15:22:42.402996: step 1100, loss = 4.45 (54.8 examples/sec; 2.337 sec/batch)
2016-01-19 15:23:09.021447: step 1110, loss = 4.59 (55.2 examples/sec; 2.319 sec/batch)
2016-01-19 15:23:32.084013: step 1120, loss = 4.42 (55.5 examples/sec; 2.306 sec/batch)
2016-01-19 15:23:55.015737: step 1130, loss = 4.33 (55.7 examples/sec; 2.298 sec/batch)
2016-01-19 15:24:18.077028: step 1140, loss = 4.30 (55.3 examples/sec; 2.314 sec/batch)
2016-01-19 15:24:41.119111: step 1150, loss = 4.24 (55.2 examples/sec; 2.317 sec/batch)
2016-01-19 15:25:04.275432: step 1160, loss = 4.22 (55.5 examples/sec; 2.307 sec/batch)
2016-01-19 15:25:27.433202: step 1170, loss = 4.18 (54.9 examples/sec; 2.331 sec/batch)
2016-01-19 15:25:50.600643: step 1180, loss = 4.15 (55.2 examples/sec; 2.317 sec/batch)
2016-01-19 15:26:13.798258: step 1190, loss = 4.13 (54.0 examples/sec; 2.372 sec/batch)
2016-01-19 15:26:37.081832: step 1200, loss = 4.07 (55.3 examples/sec; 2.314 sec/batch)
2016-01-19 15:27:02.876270: step 1210, loss = 4.06 (55.8 examples/sec; 2.295 sec/batch)
2016-01-19 15:27:26.503482: step 1220, loss = 4.00 (52.5 examples/sec; 2.436 sec/batch)
2016-01-19 15:27:49.853742: step 1230, loss = 4.00 (54.2 examples/sec; 2.364 sec/batch)
2016-01-19 15:28:14.170182: step 1240, loss = 3.95 (54.5 examples/sec; 2.347 sec/batch)
2016-01-19 15:28:37.582541: step 1250, loss = 3.91 (54.7 examples/sec; 2.339 sec/batch)
2016-01-19 15:29:01.000160: step 1260, loss = 3.89 (54.2 examples/sec; 2.360 sec/batch)
2016-01-19 15:29:24.443980: step 1270, loss = 3.85 (55.1 examples/sec; 2.325 sec/batch)
2016-01-19 15:29:47.889871: step 1280, loss = 3.84 (54.3 examples/sec; 2.357 sec/batch)
2016-01-19 15:30:11.381005: step 1290, loss = 3.84 (54.5 examples/sec; 2.348 sec/batch)
2016-01-19 15:30:34.809615: step 1300, loss = 3.75 (54.9 examples/sec; 2.331 sec/batch)
2016-01-19 15:31:00.992223: step 1310, loss = 3.76 (54.2 examples/sec; 2.362 sec/batch)
2016-01-19 15:31:24.348832: step 1320, loss = 3.73 (54.9 examples/sec; 2.334 sec/batch)
2016-01-19 15:31:47.755555: step 1330, loss = 3.71 (54.2 examples/sec; 2.363 sec/batch)
2016-01-19 15:32:11.379890: step 1340, loss = 3.63 (54.4 examples/sec; 2.355 sec/batch)
2016-01-19 15:32:34.880790: step 1350, loss = 3.62 (54.6 examples/sec; 2.344 sec/batch)
2016-01-19 15:32:58.115596: step 1360, loss = 3.62 (55.5 examples/sec; 2.308 sec/batch)
2016-01-19 15:33:22.159279: step 1370, loss = 3.55 (55.9 examples/sec; 2.288 sec/batch)
2016-01-19 15:33:45.702807: step 1380, loss = 3.52 (56.1 examples/sec; 2.281 sec/batch)
2016-01-19 15:34:09.245664: step 1390, loss = 3.49 (54.0 examples/sec; 2.371 sec/batch)
2016-01-19 15:34:32.914555: step 1400, loss = 3.53 (54.4 examples/sec; 2.351 sec/batch)
2016-01-19 15:34:59.479890: step 1410, loss = 3.45 (55.7 examples/sec; 2.298 sec/batch)
2016-01-19 15:35:22.767182: step 1420, loss = 3.47 (54.4 examples/sec; 2.354 sec/batch)
2016-01-19 15:35:46.241964: step 1430, loss = 3.39 (53.3 examples/sec; 2.401 sec/batch)
2016-01-19 15:36:09.638904: step 1440, loss = 3.36 (55.0 examples/sec; 2.326 sec/batch)
2016-01-19 15:36:33.594021: step 1450, loss = 3.33 (53.9 examples/sec; 2.377 sec/batch)
2016-01-19 15:36:57.789950: step 1460, loss = 4.34 (55.6 examples/sec; 2.301 sec/batch)
2016-01-19 15:37:20.368144: step 1470, loss = 3.61 (56.8 examples/sec; 2.253 sec/batch)
2016-01-19 15:37:43.111832: step 1480, loss = 3.37 (56.7 examples/sec; 2.257 sec/batch)
2016-01-19 15:38:05.969725: step 1490, loss = 3.31 (56.2 examples/sec; 2.279 sec/batch)
2016-01-19 15:38:28.885698: step 1500, loss = 3.32 (55.5 examples/sec; 2.305 sec/batch)
2016-01-19 15:38:54.489727: step 1510, loss = 3.28 (55.5 examples/sec; 2.305 sec/batch)
2016-01-19 15:39:17.398981: step 1520, loss = 3.19 (56.0 examples/sec; 2.284 sec/batch)
2016-01-19 15:39:40.306909: step 1530, loss = 3.16 (56.2 examples/sec; 2.278 sec/batch)
2016-01-19 15:40:03.397116: step 1540, loss = 3.19 (55.2 examples/sec; 2.319 sec/batch)
2016-01-19 15:40:26.615579: step 1550, loss = 3.10 (56.1 examples/sec; 2.283 sec/batch)
2016-01-19 15:40:49.637065: step 1560, loss = 3.09 (55.8 examples/sec; 2.294 sec/batch)
2016-01-19 15:41:12.572966: step 1570, loss = 3.06 (55.8 examples/sec; 2.293 sec/batch)
2016-01-19 15:41:35.825348: step 1580, loss = 3.04 (53.2 examples/sec; 2.408 sec/batch)
2016-01-19 15:41:59.083133: step 1590, loss = 3.00 (55.9 examples/sec; 2.291 sec/batch)
2016-01-19 15:42:22.429308: step 1600, loss = 2.97 (56.1 examples/sec; 2.283 sec/batch)
2016-01-19 15:42:48.052052: step 1610, loss = 2.98 (55.7 examples/sec; 2.297 sec/batch)
2016-01-19 15:43:11.348093: step 1620, loss = 2.92 (55.2 examples/sec; 2.318 sec/batch)
2016-01-19 15:43:34.550960: step 1630, loss = 2.91 (55.7 examples/sec; 2.296 sec/batch)
2016-01-19 15:43:58.036356: step 1640, loss = 2.89 (55.6 examples/sec; 2.301 sec/batch)
2016-01-19 15:44:21.968633: step 1650, loss = 2.86 (54.8 examples/sec; 2.334 sec/batch)
2016-01-19 15:44:45.522487: step 1660, loss = 2.84 (55.9 examples/sec; 2.291 sec/batch)
2016-01-19 15:45:09.248101: step 1670, loss = 2.81 (55.1 examples/sec; 2.324 sec/batch)
2016-01-19 15:45:33.059973: step 1680, loss = 2.95 (53.2 examples/sec; 2.405 sec/batch)
2016-01-19 15:45:56.084235: step 1690, loss = 2.81 (55.5 examples/sec; 2.307 sec/batch)
2016-01-19 15:46:19.126445: step 1700, loss = 2.79 (53.8 examples/sec; 2.380 sec/batch)
2016-01-19 15:46:45.101182: step 1710, loss = 2.72 (56.5 examples/sec; 2.266 sec/batch)
2016-01-19 15:47:08.016564: step 1720, loss = 2.71 (55.7 examples/sec; 2.296 sec/batch)
2016-01-19 15:47:31.246231: step 1730, loss = 2.75 (55.8 examples/sec; 2.296 sec/batch)
2016-01-19 15:47:54.149466: step 1740, loss = 2.66 (55.7 examples/sec; 2.298 sec/batch)
2016-01-19 15:48:17.042010: step 1750, loss = 2.64 (55.7 examples/sec; 2.298 sec/batch)
2016-01-19 15:48:39.989497: step 1760, loss = 2.63 (56.1 examples/sec; 2.282 sec/batch)
2016-01-19 15:49:02.938406: step 1770, loss = 2.59 (55.5 examples/sec; 2.307 sec/batch)
2016-01-19 15:49:25.986021: step 1780, loss = 2.58 (55.5 examples/sec; 2.305 sec/batch)
2016-01-19 15:49:49.049956: step 1790, loss = 2.56 (55.5 examples/sec; 2.305 sec/batch)
2016-01-19 15:50:12.217206: step 1800, loss = 2.56 (56.0 examples/sec; 2.284 sec/batch)
2016-01-19 15:50:37.927270: step 1810, loss = 2.53 (55.5 examples/sec; 2.306 sec/batch)
2016-01-19 15:51:01.093218: step 1820, loss = 2.63 (55.6 examples/sec; 2.302 sec/batch)
2016-01-19 15:51:24.017176: step 1830, loss = 2.49 (55.7 examples/sec; 2.297 sec/batch)
2016-01-19 15:51:46.964287: step 1840, loss = 2.47 (55.8 examples/sec; 2.295 sec/batch)
2016-01-19 15:52:09.965600: step 1850, loss = 2.45 (55.2 examples/sec; 2.318 sec/batch)
2016-01-19 15:52:32.958764: step 1860, loss = 2.43 (55.8 examples/sec; 2.296 sec/batch)
2016-01-19 15:52:56.098574: step 1870, loss = 2.40 (55.6 examples/sec; 2.301 sec/batch)
2016-01-19 15:53:19.143205: step 1880, loss = 2.39 (55.5 examples/sec; 2.305 sec/batch)
2016-01-19 15:53:42.203902: step 1890, loss = 2.36 (55.1 examples/sec; 2.324 sec/batch)
2016-01-19 15:54:05.344967: step 1900, loss = 2.35 (55.6 examples/sec; 2.304 sec/batch)
2016-01-19 15:54:33.870385: step 1910, loss = 3.51 (56.6 examples/sec; 2.260 sec/batch)
2016-01-19 15:54:56.462159: step 1920, loss = 2.85 (56.2 examples/sec; 2.276 sec/batch)
2016-01-19 15:55:18.974570: step 1930, loss = 2.55 (57.0 examples/sec; 2.247 sec/batch)
2016-01-19 15:55:41.510815: step 1940, loss = 2.73 (56.5 examples/sec; 2.267 sec/batch)
2016-01-19 15:56:04.087491: step 1950, loss = 2.59 (56.5 examples/sec; 2.265 sec/batch)
2016-01-19 15:56:26.725022: step 1960, loss = 2.41 (56.4 examples/sec; 2.269 sec/batch)
2016-01-19 15:56:49.283446: step 1970, loss = 2.35 (56.7 examples/sec; 2.256 sec/batch)
2016-01-19 15:57:11.879110: step 1980, loss = 2.27 (56.7 examples/sec; 2.256 sec/batch)
2016-01-19 15:57:34.450237: step 1990, loss = 2.36 (56.6 examples/sec; 2.260 sec/batch)
2016-01-19 15:57:57.147176: step 2000, loss = 2.25 (56.1 examples/sec; 2.280 sec/batch)
2016-01-19 15:58:22.614293: step 2010, loss = 2.22 (57.1 examples/sec; 2.242 sec/batch)
2016-01-19 15:58:45.374274: step 2020, loss = 2.24 (56.3 examples/sec; 2.272 sec/batch)
2016-01-19 15:59:08.198355: step 2030, loss = 2.17 (55.9 examples/sec; 2.291 sec/batch)
2016-01-19 15:59:30.998246: step 2040, loss = 2.21 (56.1 examples/sec; 2.282 sec/batch)
2016-01-19 15:59:53.775694: step 2050, loss = 2.14 (56.3 examples/sec; 2.272 sec/batch)
2016-01-19 16:00:16.431515: step 2060, loss = 2.15 (56.2 examples/sec; 2.279 sec/batch)
2016-01-19 16:00:39.133519: step 2070, loss = 2.12 (56.5 examples/sec; 2.266 sec/batch)
2016-01-19 16:01:01.904606: step 2080, loss = 2.09 (56.3 examples/sec; 2.273 sec/batch)
2016-01-19 16:01:24.616518: step 2090, loss = 2.05 (55.6 examples/sec; 2.301 sec/batch)
2016-01-19 16:01:47.283670: step 2100, loss = 2.05 (56.2 examples/sec; 2.276 sec/batch)
2016-01-19 16:02:12.580789: step 2110, loss = 2.04 (56.1 examples/sec; 2.280 sec/batch)
2016-01-19 16:02:35.397591: step 2120, loss = 2.03 (56.4 examples/sec; 2.271 sec/batch)
2016-01-19 16:02:58.215794: step 2130, loss = 1.99 (56.2 examples/sec; 2.278 sec/batch)
2016-01-19 16:03:21.009617: step 2140, loss = 1.96 (56.3 examples/sec; 2.273 sec/batch)
2016-01-19 16:03:43.761206: step 2150, loss = 1.96 (57.0 examples/sec; 2.247 sec/batch)
2016-01-19 16:04:06.378963: step 2160, loss = 2.08 (57.0 examples/sec; 2.246 sec/batch)
2016-01-19 16:04:28.806400: step 2170, loss = 2.37 (57.3 examples/sec; 2.232 sec/batch)
2016-01-19 16:04:51.693084: step 2180, loss = 2.13 (57.1 examples/sec; 2.240 sec/batch)
2016-01-19 16:05:14.170734: step 2190, loss = 2.04 (57.1 examples/sec; 2.243 sec/batch)
2016-01-19 16:05:36.640979: step 2200, loss = 2.01 (57.0 examples/sec; 2.244 sec/batch)
2016-01-19 16:06:01.731761: step 2210, loss = 1.90 (56.3 examples/sec; 2.275 sec/batch)
2016-01-19 16:06:25.720114: step 2220, loss = 2.06 (56.8 examples/sec; 2.255 sec/batch)
2016-01-19 16:06:48.257329: step 2230, loss = 1.88 (56.6 examples/sec; 2.260 sec/batch)
2016-01-19 16:07:10.826929: step 2240, loss = 1.88 (56.6 examples/sec; 2.261 sec/batch)
2016-01-19 16:07:33.346796: step 2250, loss = 1.91 (56.3 examples/sec; 2.274 sec/batch)
2016-01-19 16:07:56.802585: step 2260, loss = 1.85 (55.3 examples/sec; 2.314 sec/batch)
2016-01-19 16:08:19.484501: step 2270, loss = 1.88 (56.7 examples/sec; 2.259 sec/batch)
2016-01-19 16:08:42.121354: step 2280, loss = 1.81 (56.1 examples/sec; 2.280 sec/batch)
2016-01-19 16:09:04.837793: step 2290, loss = 1.78 (56.4 examples/sec; 2.270 sec/batch)
2016-01-19 16:09:27.532550: step 2300, loss = 1.81 (56.0 examples/sec; 2.287 sec/batch)
2016-01-19 16:09:52.844850: step 2310, loss = 1.76 (57.1 examples/sec; 2.241 sec/batch)
2016-01-19 16:10:15.576564: step 2320, loss = 1.73 (56.2 examples/sec; 2.277 sec/batch)
2016-01-19 16:10:38.296577: step 2330, loss = 1.71 (56.4 examples/sec; 2.270 sec/batch)
2016-01-19 16:11:01.070134: step 2340, loss = 1.71 (56.0 examples/sec; 2.284 sec/batch)
2016-01-19 16:11:23.830971: step 2350, loss = 1.85 (56.5 examples/sec; 2.266 sec/batch)
2016-01-19 16:11:46.587011: step 2360, loss = 1.74 (56.2 examples/sec; 2.279 sec/batch)
2016-01-19 16:12:09.362380: step 2370, loss = 1.66 (56.1 examples/sec; 2.280 sec/batch)
2016-01-19 16:12:32.141005: step 2380, loss = 1.67 (56.2 examples/sec; 2.277 sec/batch)
2016-01-19 16:12:54.927388: step 2390, loss = 1.66 (56.3 examples/sec; 2.273 sec/batch)
2016-01-19 16:13:17.763204: step 2400, loss = 1.65 (55.9 examples/sec; 2.290 sec/batch)
2016-01-19 16:13:43.142924: step 2410, loss = 1.65 (56.5 examples/sec; 2.266 sec/batch)
2016-01-19 16:14:05.929911: step 2420, loss = 2.08 (56.4 examples/sec; 2.269 sec/batch)
2016-01-19 16:14:28.510742: step 2430, loss = 1.64 (56.6 examples/sec; 2.261 sec/batch)
2016-01-19 16:14:51.060620: step 2440, loss = 1.58 (56.9 examples/sec; 2.250 sec/batch)
2016-01-19 16:15:13.613217: step 2450, loss = 1.58 (56.5 examples/sec; 2.265 sec/batch)
2016-01-19 16:15:36.117401: step 2460, loss = 1.55 (56.9 examples/sec; 2.249 sec/batch)
2016-01-19 16:15:58.727251: step 2470, loss = 1.57 (56.2 examples/sec; 2.278 sec/batch)
2016-01-19 16:16:21.290712: step 2480, loss = 1.52 (57.2 examples/sec; 2.240 sec/batch)
2016-01-19 16:16:43.879004: step 2490, loss = 1.52 (56.5 examples/sec; 2.265 sec/batch)
2016-01-19 16:17:06.349949: step 2500, loss = 1.49 (56.7 examples/sec; 2.258 sec/batch)
2016-01-19 16:17:31.405502: step 2510, loss = 1.56 (57.2 examples/sec; 2.238 sec/batch)
2016-01-19 16:17:53.903773: step 2520, loss = 1.48 (56.5 examples/sec; 2.267 sec/batch)
2016-01-19 16:18:16.491100: step 2530, loss = 1.46 (56.5 examples/sec; 2.264 sec/batch)
2016-01-19 16:18:38.976749: step 2540, loss = 1.46 (57.1 examples/sec; 2.243 sec/batch)
2016-01-19 16:19:01.562046: step 2550, loss = 1.44 (56.7 examples/sec; 2.259 sec/batch)
2016-01-19 16:19:24.074997: step 2560, loss = 1.43 (56.8 examples/sec; 2.254 sec/batch)
2016-01-19 16:19:46.678030: step 2570, loss = 1.42 (56.6 examples/sec; 2.263 sec/batch)
2016-01-19 16:20:09.285307: step 2580, loss = 1.40 (56.6 examples/sec; 2.262 sec/batch)
2016-01-19 16:20:31.760793: step 2590, loss = 1.39 (56.8 examples/sec; 2.253 sec/batch)
2016-01-19 16:20:54.423011: step 2600, loss = 1.38 (56.8 examples/sec; 2.254 sec/batch)
2016-01-19 16:21:19.667527: step 2610, loss = 1.37 (55.9 examples/sec; 2.289 sec/batch)
2016-01-19 16:21:42.487451: step 2620, loss = 1.36 (54.6 examples/sec; 2.346 sec/batch)
2016-01-19 16:22:05.259092: step 2630, loss = 1.35 (56.6 examples/sec; 2.260 sec/batch)
2016-01-19 16:22:27.876294: step 2640, loss = 1.34 (56.6 examples/sec; 2.262 sec/batch)
2016-01-19 16:22:50.595526: step 2650, loss = 1.34 (56.5 examples/sec; 2.266 sec/batch)
2016-01-19 16:23:13.272212: step 2660, loss = 1.32 (56.2 examples/sec; 2.276 sec/batch)
2016-01-19 16:23:37.678827: step 2670, loss = 1.31 (55.2 examples/sec; 2.318 sec/batch)
2016-01-19 16:24:01.299547: step 2680, loss = 1.29 (54.1 examples/sec; 2.367 sec/batch)
2016-01-19 16:24:26.100612: step 2690, loss = 1.31 (48.0 examples/sec; 2.667 sec/batch)
2016-01-19 16:24:49.638371: step 2700, loss = 1.39 (51.4 examples/sec; 2.492 sec/batch)
2016-01-19 16:25:16.178662: step 2710, loss = 1.28 (54.8 examples/sec; 2.337 sec/batch)
2016-01-19 16:25:41.210352: step 2720, loss = 1.27 (55.8 examples/sec; 2.294 sec/batch)
2016-01-19 16:26:04.695206: step 2730, loss = 1.25 (55.1 examples/sec; 2.325 sec/batch)
2016-01-19 16:26:28.125682: step 2740, loss = 1.24 (55.5 examples/sec; 2.306 sec/batch)
2016-01-19 16:26:50.800431: step 2750, loss = 1.23 (56.3 examples/sec; 2.274 sec/batch)
2016-01-19 16:27:13.580184: step 2760, loss = 1.22 (56.5 examples/sec; 2.266 sec/batch)
2016-01-19 16:27:36.311089: step 2770, loss = 1.21 (56.4 examples/sec; 2.269 sec/batch)
2016-01-19 16:27:59.020801: step 2780, loss = 1.20 (56.2 examples/sec; 2.279 sec/batch)
2016-01-19 16:28:21.905438: step 2790, loss = 1.19 (54.7 examples/sec; 2.340 sec/batch)
2016-01-19 16:28:44.918403: step 2800, loss = 1.18 (55.4 examples/sec; 2.311 sec/batch)
2016-01-19 16:29:10.549953: step 2810, loss = 1.17 (55.6 examples/sec; 2.302 sec/batch)
2016-01-19 16:29:33.702876: step 2820, loss = 1.16 (56.1 examples/sec; 2.282 sec/batch)
2016-01-19 16:29:56.727920: step 2830, loss = 1.15 (55.7 examples/sec; 2.297 sec/batch)
2016-01-19 16:30:19.702005: step 2840, loss = 1.14 (55.7 examples/sec; 2.299 sec/batch)
2016-01-19 16:30:42.748566: step 2850, loss = 1.14 (54.6 examples/sec; 2.345 sec/batch)
2016-01-19 16:31:06.014386: step 2860, loss = 1.12 (55.8 examples/sec; 2.294 sec/batch)
2016-01-19 16:31:29.064615: step 2870, loss = 1.11 (55.7 examples/sec; 2.297 sec/batch)
2016-01-19 16:31:52.545651: step 2880, loss = 1.11 (49.9 examples/sec; 2.567 sec/batch)
2016-01-19 16:32:16.751102: step 2890, loss = 1.10 (53.1 examples/sec; 2.411 sec/batch)
2016-01-19 16:32:40.578252: step 2900, loss = 1.09 (54.7 examples/sec; 2.342 sec/batch)
2016-01-19 16:33:06.726546: step 2910, loss = 1.09 (54.6 examples/sec; 2.345 sec/batch)
2016-01-19 16:33:30.192852: step 2920, loss = 1.86 (55.9 examples/sec; 2.291 sec/batch)
2016-01-19 16:33:53.013532: step 2930, loss = 1.63 (56.0 examples/sec; 2.287 sec/batch)
2016-01-19 16:34:15.718065: step 2940, loss = 1.32 (56.3 examples/sec; 2.273 sec/batch)
2016-01-19 16:34:38.412387: step 2950, loss = 1.43 (56.7 examples/sec; 2.258 sec/batch)
2016-01-19 16:35:02.319039: step 2960, loss = 1.10 (54.7 examples/sec; 2.340 sec/batch)
2016-01-19 16:35:25.698944: step 2970, loss = 1.07 (56.2 examples/sec; 2.279 sec/batch)
2016-01-19 16:35:48.529542: step 2980, loss = 1.07 (56.3 examples/sec; 2.272 sec/batch)
2016-01-19 16:36:11.327106: step 2990, loss = 1.04 (55.8 examples/sec; 2.294 sec/batch)
2016-01-19 16:36:34.416702: step 3000, loss = 1.02 (55.4 examples/sec; 2.312 sec/batch)